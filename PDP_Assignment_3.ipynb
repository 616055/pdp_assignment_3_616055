{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PDP Assignment 3",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9gWtDNmeWuwfQNZL9Pfol",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/616055/pdp_assignment_3_616055/blob/master/PDP_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvAGPd5b7nJO",
        "colab_type": "text"
      },
      "source": [
        "**Setup environment for Spark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m62TKZsRgFE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.0-preview2/spark-3.0.0-preview2-bin-hadoop3.2.tgz\n",
        "!tar -xvf spark-3.0.0-preview2-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "import os \n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-preview2-bin-hadoop3.2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7HygYD17rgu",
        "colab_type": "text"
      },
      "source": [
        "**Load data and map the columns with Spark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZuWMsAlf2bD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import packages\n",
        "import findspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# start the session\n",
        "findspark.init()\n",
        "spark = SparkSession.builder \\\n",
        "            .master(\"local\") \\\n",
        "            .appName(\"Titanic\") \\\n",
        "            .config(\"spark.executor.memory\", \"1gb\") \\\n",
        "            .getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# load the titanic text file\n",
        "rdd = sc.textFile(\"titanic.csv\")\n",
        "# get the header line (first line)\n",
        "header = rdd.first()\n",
        "# get all rows which are not the header\n",
        "rdd = rdd.filter(lambda line: line != header)\n",
        "# split the values by a comma\n",
        "rdd = rdd.map(lambda line: line.split(\",\"))\n",
        "# define the headers\n",
        "df = rdd.map(lambda line: Row(\n",
        "    survived=line[0],\n",
        "    pclass=line[1],\n",
        "    name=line[2],\n",
        "    sex=line[3],\n",
        "    age=line[4],\n",
        "    siblings_spoused_aboard=line[5],\n",
        "    parents_children_aboard=line[6],\n",
        "    fare=line[7]\n",
        ")).toDF()\n",
        "# convert age values to integers\n",
        "df = df.withColumn(\"age\", df[\"age\"].cast(IntegerType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKkQEFLvXPU4",
        "colab_type": "text"
      },
      "source": [
        "**Assignment 3a**\n",
        "*Calculate the conditional probability that a person survives given their sex and passenger-class*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VebdfUwLScqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculateProbability(df, sex, pclass):\n",
        "  # where conditions for sex and passenger class\n",
        "  filtered = df.rdd.filter(lambda line: line['sex'] == sex and line['pclass'] == pclass)\n",
        "\n",
        "  # get the amount of survived and not survived persons\n",
        "  survived = filtered.filter(lambda line: line['survived'] == '1').collect()\n",
        "  not_survived = filtered.filter(lambda line: line['survived'] == '0').collect()\n",
        "\n",
        "  # convert it back to a dataframe\n",
        "  survived_df = spark.createDataFrame(survived)\n",
        "  not_survived_df = spark.createDataFrame(not_survived)\n",
        "\n",
        "  # get the amount of persons for survived and the total persons\n",
        "  survived_persons = survived_df.count()\n",
        "  total_persons = survived_persons + not_survived_df.count()\n",
        "\n",
        "  return survived_persons, total_persons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONo0mXrbZvav",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# P( S = true | G = female, C = 1 )\n",
        "df1 = calculateProbability(df, 'female', '1')\n",
        "print(\"Conditional probability of P( S = true | G = female, C = 1 ) = \" + str(df1[0]) + \" / \" + str(df1[1]))\n",
        "\n",
        "# P( S = true | G = female, C = 2 )\n",
        "df2 = calculateProbability(df, 'female', '2')\n",
        "print(\"Conditional probability of P( S = true | G = female, C = 2 ) = \" + str(df2[0]) + \" / \" + str(df2[1]))\n",
        "\n",
        "# P( S = true | G = female, C = 2 )\n",
        "df3 = calculateProbability(df, 'female', '3')\n",
        "print(\"Conditional probability of P( S = true | G = female, C = 3 ) = \" + str(df3[0]) + \" / \" + str(df3[1]))\n",
        "\n",
        "# P( S = true | G = male, C = 1 )\n",
        "df4 = calculateProbability(df, 'male', '1')\n",
        "print(\"Conditional probability of P( S = true | G = male, C = 1 ) = \" + str(df4[0]) + \" / \" + str(df4[1]))\n",
        "\n",
        "# P( S = true | G = male, C = 2 )\n",
        "df5 = calculateProbability(df, 'male', '2')\n",
        "print(\"Conditional probability of P( S = true | G = male, C = 2 ) = \" + str(df5[0]) + \" / \" + str(df5[1]))\n",
        "\n",
        "# P( S = true | G = male, C = 3 )\n",
        "df6 = calculateProbability(df, 'male', '3')\n",
        "print(\"Conditional probability of P( S = true | G = male, C = 3 ) = \" + str(df6[0]) + \" / \" + str(df6[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR_ioIAsGy0v",
        "colab_type": "text"
      },
      "source": [
        "Conditional probability of P( S = true | G = female, C = 1 ) = 91 / 94<br/>\n",
        "Conditional probability of P( S = true | G = female, C = 2 ) = 70 / 76<>\n",
        "Conditional probability of P( S = true | G = female, C = 3 ) = 72 / 144\n",
        "Conditional probability of P( S = true | G = male, C = 1 ) = 45 / 122\n",
        "Conditional probability of P( S = true | G = male, C = 2 ) = 17 / 108\n",
        "Conditional probability of P( S = true | G = male, C = 3 ) = 47 / 343"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqWNud7q5MNU",
        "colab_type": "text"
      },
      "source": [
        "**Assignment 3b**\n",
        "*What is the probability that a child who is in third class and is 10 years old or younger survives?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv-XgAN8C0Hk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# where conditions for age and passenger class\n",
        "filtered = df.rdd.filter(lambda line: line['age'] <= 10 and line['pclass'] == '3')\n",
        "\n",
        "# get the amount of survived and not survived persons\n",
        "survived = filtered.filter(lambda line: line['survived'] == '1').collect()\n",
        "not_survived = filtered.filter(lambda line: line['survived'] == '0').collect()\n",
        "\n",
        "# convert it back to a dataframe\n",
        "survived_df = spark.createDataFrame(survived)\n",
        "not_survived_df = spark.createDataFrame(not_survived)\n",
        "\n",
        "# get the amount of persons for survived and the total persons\n",
        "survived_persons = survived_df.count()\n",
        "total_persons = survived_persons + not_survived_df.count()\n",
        "\n",
        "percentage = survived_persons / total_persons * 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRdF5eA-5ncq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# P( S = true | A ≤ 10, C = 3 )\n",
        "print(\"P( S = true | A ≤ 10, C = 3 ) = \" + str(survived_persons) + \" / \" + str(total_persons) + \" = \" + str(percentage) + \"%\")\n",
        "print(\"Beta(⍺=\" + str(total_persons + 1) + \", β=\" + str(total_persons + 1) + \")\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKDbu_m0GUJJ",
        "colab_type": "text"
      },
      "source": [
        "P( S = true | A ≤ 10, C = 3 ) = 22 / 53 = 41.509433962264154%<br/>\n",
        "Beta(⍺=54, β=54)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3d3W1pGG1Ih",
        "colab_type": "text"
      },
      "source": [
        "**Assignment 3c**\n",
        "*How much did people pay to be on the ship?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWVKAvghG4Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculateExpectation(df, pclass):\n",
        "  # where condition for passenger class\n",
        "  filtered = df.rdd.filter(lambda line: line['pclass'] == pclass)\n",
        "  # convert it back to a dataframe\n",
        "  filtered_df = spark.createDataFrame(filtered)\n",
        "  # calculate the average of the paid fare\n",
        "  expectation = filtered_df.groupBy('pclass').avg().collect()\n",
        "\n",
        "  return expectation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUwfZLg4I4PM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# E[ X | C = 1 ]\n",
        "expectation1 = calculateExpectation(df, '1')\n",
        "print(\"E[ X | C = 1 ] = \" + str(expectation1[0][1]) + \"%\");\n",
        "\n",
        "# E[ X | C = 2 ]\n",
        "expectation2 = calculateExpectation(df, '2')\n",
        "print(\"E[ X | C = 2 ] = \" + str(expectation2[0][1]) + \"%\");\n",
        "\n",
        "# E[ X | C = 3 ]\n",
        "expectation3 = calculateExpectation(df, '3')\n",
        "print(\"E[ X | C = 3 ] = \" + str(expectation3[0][1]) + \"%\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2NihIIJKpEq",
        "colab_type": "text"
      },
      "source": [
        "E[ X | C = 1 ] = 38.782407407407405%<br/>\n",
        "E[ X | C = 2 ] = 29.847826086956523%<br/>\n",
        "E[ X | C = 3 ] = 25.170431211498972%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIHlpwPaOX_7",
        "colab_type": "text"
      },
      "source": [
        "**End session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oODzkTcCOaBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}